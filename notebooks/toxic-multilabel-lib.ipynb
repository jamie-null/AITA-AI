{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertConfig, BertForMaskedLM, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from fastai.text import Tokenizer, Vocab\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "#import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import datetime\n",
    "    \n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "from fast_bert.modeling import BertForMultiLabelSequenceClassification\n",
    "from fast_bert.data import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\n",
    "from fast_bert.learner import BertLearner\n",
    "from fast_bert.metrics import accuracy, accuracy_thresh, fbeta, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('../data/')\n",
    "LABEL_PATH = DATA_PATH#Path('../labels/')\n",
    "\n",
    "AUG_DATA_PATH = Path('../data/data_augmentation/')\n",
    "\n",
    "MODEL_PATH=Path('../models/')\n",
    "LOG_PATH=Path('../logs/')\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "model_state_dict = None\n",
    "\n",
    "# BERT_PRETRAINED_PATH = Path('../../bert_models/pretrained-weights/cased_L-12_H-768_A-12/')\n",
    "BERT_PRETRAINED_PATH = Path('../../BERT/PREAITA')#../BERT/uncased_L-12_H-768_A-12/')\n",
    "# BERT_PRETRAINED_PATH = Path('../../bert_fastai/pretrained-weights/uncased_L-24_H-1024_A-16/')\n",
    "# FINETUNED_PATH = Path('../models/finetuned_model.bin')\n",
    "FINETUNED_PATH = None\n",
    "# model_state_dict = torch.load(FINETUNED_PATH)\n",
    "\n",
    "LOG_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"run_text\": \"AITA-AI\",\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"log_path\": LOG_PATH,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": DATA_PATH,\n",
    "    \"task_name\": \"AITA-AI\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": MODEL_PATH/'output',\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"learning_rate\": 5e-6,\n",
    "    \"num_train_epochs\": 1.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 69,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    #VERY IMPORTANT\n",
    "    \"optimize_on_cpu\": True,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "    handlers=[\n",
    "        logging.FileHandler(logfile),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ])\n",
    "\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2019 02:52:33 - INFO - root -   {'run_text': 'AITA-AI', 'train_size': -1, 'val_size': -1, 'log_path': WindowsPath('../logs'), 'full_data_dir': WindowsPath('../data'), 'data_dir': WindowsPath('../data'), 'task_name': 'AITA-AI', 'no_cuda': False, 'bert_model': WindowsPath('../../BERT/PREAITA'), 'output_dir': WindowsPath('../models/output'), 'max_seq_length': 512, 'do_train': True, 'do_eval': True, 'do_lower_case': True, 'train_batch_size': 16, 'eval_batch_size': 16, 'learning_rate': 5e-06, 'num_train_epochs': 1.0, 'warmup_proportion': 0.1, 'local_rank': -1, 'seed': 69, 'gradient_accumulation_steps': 8, 'optimize_on_cpu': True, 'fp16': False, 'loss_scale': 128}\n"
     ]
    }
   ],
   "source": [
    "logger.info(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/26/2019 02:52:34 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ..\\..\\BERT\\PREAITA\\vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_PATH, do_lower_case=args['do_lower_case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "if torch.cuda.device_count() > 1:\n",
    "    multi_gpu = True\n",
    "else:\n",
    "    multi_gpu = False\n",
    "multi_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label    2\\nName: 0, dtype: object'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-eac3875ec81b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                           \u001b[0mtext_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                           \u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_seq_length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                           multi_gpu=multi_gpu, multi_label=False)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdatabunch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fast_bert\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data_dir, label_dir, tokenizer, train_file, val_file, test_data, label_file, text_col, label_col, bs, maxlen, multi_gpu, multi_label, backend)\u001b[0m\n\u001b[0;32m    352\u001b[0m                 train_file, text_col=text_col, label_col=label_col)\n\u001b[0;32m    353\u001b[0m             train_features = convert_examples_to_features(train_examples, label_list=self.labels,\n\u001b[1;32m--> 354\u001b[1;33m                                                           tokenizer=tokenizer, max_seq_length=maxlen)\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m             all_input_ids = torch.tensor(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fast_bert\\data.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[1;34m(examples, label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                 \u001b[0mlabel_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[0mlabel_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'label    2\\nName: 0, dtype: object'"
     ]
    }
   ],
   "source": [
    "databunch = BertDataBunch(args['data_dir'], LABEL_PATH, tokenizer, train_file='train.csv', val_file='val.csv',\n",
    "                          test_data='test.csv',\n",
    "                          text_col=\"text\", label_col=label_cols,\n",
    "                          bs=args['train_batch_size'], maxlen=args['max_seq_length'], \n",
    "                          multi_gpu=multi_gpu, multi_label=False)\n",
    "databunch.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#databunch = BertDataBunch.load(args['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = len(databunch.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributed.init_process_group(backend=\"nccl\", \n",
    "#                                      init_method = \"tcp://localhost:23459\", \n",
    "#                                      rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\n",
    "metrics.append({'name': 'roc_auc', 'function': roc_auc})\n",
    "metrics.append({'name': 'fbeta', 'function': fbeta})\n",
    "metrics.append({'name': 'accuracy_single', 'function': accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learner\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner = BertLearner.from_pretrained_model(databunch, BERT_PRETRAINED_PATH, metrics, device, logger, \n",
    "                                            finetuned_wgts_path=FINETUNED_PATH, \n",
    "                                            is_fp16=args['fp16'], loss_scale=args['loss_scale'], \n",
    "                                            multi_gpu=multi_gpu,  multi_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learner.fit(1, lr=args['learning_rate'], schedule_type=\"warmup_cosine_hard_restarts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.save_and_reload(MODEL_PATH,'bert_uncased_fullypretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
